{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08fae2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3d89b48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a1e997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_client import ChatMessage, MessageRole, Prompt, LLMResponse, call_model, ModelClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d86a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_ok = \"Qwen/Qwen3-Next-80B-A3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc290f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll3p2_3b = \"meta-llama/Llama-3.2-3B-Instruct-Turbo\"\n",
    "gemma_3n = \"google/gemma-3n-e2b-it:free\"\n",
    "cheap_model = gemma_3n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7b3c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_base = \"meta-llama/llama-3.1-405b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbfc3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = ChatMessage(role=MessageRole.USER, content=\"Hello, world!\")\n",
    "out = await call_model(ll_base, [message], logprobs=True, max_tokens=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3d0fdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResponse(model_id='meta-llama/llama-3.1-405b', completion='', stop_reason=None, cost=None, duration=1.2952570915222168, api_duration=1.2949938774108887, usage={'prompt_tokens': 11, 'completion_tokens': 0, 'total_tokens': 11}, finish_reason=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d529b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_logprobs(model_id: str):\n",
    "    client = ModelClient()\n",
    "    \n",
    "    messages = [\n",
    "        ChatMessage(role=MessageRole.USER, content=\"Answer with True or False: Is 2+2=4?\")\n",
    "    ]\n",
    "    \n",
    "    response = await client.openrouter_client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=[{\"role\": m.role.value, \"content\": m.content} for m in messages],\n",
    "        max_tokens=2,\n",
    "        temperature=0,\n",
    "        logprobs=True,\n",
    "        top_logprobs=5\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f840805",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await test_logprobs(ll_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41d28b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='gen-1762255735-yYArQydtTFDuk467eMeB', choices=[Choice(finish_reason=None, index=0, logprobs=None, message=ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason=None)], created=1762255735, model='meta-llama/llama-3.1-405b', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=0, prompt_tokens=21, total_tokens=21, completion_tokens_details=None, prompt_tokens_details=None), provider='Hyperbolic')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "08524f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If above still gives 0 tokens, try instruct model with direct question:\n",
    "\n",
    "client = ModelClient()\n",
    "response2 = await client.openrouter_client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.1-405b-instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say only: True\"}],\n",
    "    max_tokens=5,\n",
    "    temperature=0,\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "52c007dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='gen-1762256640-AIbY9CJICTYUQhOOhuSO', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='True', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1762256640, model='meta-llama/llama-3.1-405b-instruct', object='chat.completion', service_tier=None, system_fingerprint='', usage=CompletionUsage(completion_tokens=2, prompt_tokens=14, total_tokens=16, completion_tokens_details=None, prompt_tokens_details=None), provider='Hyperbolic')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "382b39d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      " False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "system_content = \"You are a gourmet. Be descriptive and helpful.\"\n",
    "user_content = \"Tell me about Chinese hotpot\"\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"HYPERBOLIC_API_KEY\"),\n",
    "    base_url=\"https://api.hyperbolic.xyz/v1\",\n",
    "    )\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    messages=[\n",
    "        #{\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": \"Does 1+2=4? Answer with True or False.\"},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=2,\n",
    "    logprobs=True,\n",
    "    top_logprobs=5,\n",
    ")\n",
    "\n",
    "response = chat_completion.choices[0].message.content\n",
    "print(\"Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4ca9d4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='oHuh2F8-4YNCb4-9993f878293f5012', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='False', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None))], created=1762257832, model='meta-llama/Meta-Llama-3.1-70B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=2, prompt_tokens=49, total_tokens=51, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "65d4aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_msgs =[\n",
    "        #{\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": \"Does 1+2=4? Answer with True or False.\"},\n",
    "    ],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "299e36d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyperbolic API wrapper with logprobs support via raw REST API.\n",
    "\"\"\"\n",
    "import os\n",
    "import httpx\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "\n",
    "class HyperbolicLogprobsClient:\n",
    "    \"\"\"Async client for Hyperbolic API with logprobs extraction.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        self.api_key = api_key or os.getenv(\"HYPERBOLIC_API_KEY\")\n",
    "        self.base_url = \"https://api.hyperbolic.xyz/v1\"\n",
    "        self.client = httpx.AsyncClient(timeout=60.0)\n",
    "    \n",
    "    async def chat_completion_raw(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        model: str,\n",
    "        max_tokens: int = 1,\n",
    "        temperature: float = 0.0,\n",
    "        # logprobs: bool = True,\n",
    "        # top_logprobs: int = 2,\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Raw chat completion call - returns full JSON response.\n",
    "        \n",
    "        Args:\n",
    "            messages: Chat messages in OpenAI format\n",
    "            model: Model ID\n",
    "            max_tokens: Max tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "            logprobs: Number of logprobs to return (integer!)\n",
    "            top_logprobs: Number of top alternatives\n",
    "            **kwargs: Additional API parameters\n",
    "            \n",
    "        Returns:\n",
    "            Full JSON response from API\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"messages\": messages,\n",
    "            \"model\": model,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            # \"logprobs\": logprobs,\n",
    "            #\"top_logprobs\": top_logprobs,\n",
    "            \"stream\": False,\n",
    "            **kwargs,\n",
    "        }\n",
    "        \n",
    "        \n",
    "        response = await self.client.post(\n",
    "            f\"{self.base_url}/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    def extract_label_logprobs(\n",
    "        self,\n",
    "        response_data: Dict[str, Any],\n",
    "        labels: List[str] = [\"True\", \"False\"],\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Extract log probabilities for specific labels from API response.\n",
    "        \n",
    "        Args:\n",
    "            response_data: Raw JSON response from chat_completion_raw()\n",
    "            labels: Labels to extract probabilities for\n",
    "            \n",
    "        Returns:\n",
    "            Dict mapping label -> log probability\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If logprobs cannot be extracted\n",
    "        \"\"\"\n",
    "        logprobs_data = response_data[\"choices\"][0][\"logprobs\"]\n",
    "        \n",
    "        if logprobs_data is None:\n",
    "            raise ValueError(\"API returned logprobs=None\")\n",
    "        \n",
    "        # Get first token's logprobs - try different formats\n",
    "        if \"content\" in logprobs_data:\n",
    "            # OpenAI-style format\n",
    "            token_logprobs = logprobs_data[\"content\"][0][\"top_logprobs\"]\n",
    "        elif \"top_logprobs\" in logprobs_data:\n",
    "            # Alternative format\n",
    "            token_logprobs = logprobs_data[\"top_logprobs\"][0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected logprobs format: {logprobs_data}\")\n",
    "        \n",
    "        # Build dict mapping label -> logprob\n",
    "        result = {}\n",
    "        for item in token_logprobs:\n",
    "            token = item[\"token\"].strip()\n",
    "            if token in labels:\n",
    "                result[token] = item[\"logprob\"]\n",
    "        \n",
    "        # Check if we got all labels\n",
    "        if len(result) != len(labels):\n",
    "            missing = set(labels) - set(result.keys())\n",
    "            raise ValueError(f\"Missing logprobs for labels: {missing}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    async def get_label_logprobs(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        model: str,\n",
    "        labels: List[str] = [\"True\", \"False\"],\n",
    "        **kwargs,\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Convenience method: get logprobs for labels in one call.\n",
    "        \"\"\"\n",
    "        response = await self.chat_completion_raw(messages, model, **kwargs)\n",
    "        return self.extract_label_logprobs(response, labels)\n",
    "    \n",
    "    async def close(self):\n",
    "        \"\"\"Close the HTTP client.\"\"\"\n",
    "        await self.client.aclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1fda2dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPStatusError",
     "evalue": "Client error '422 Unprocessable Entity' for url 'https://api.hyperbolic.xyz/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m client = HyperbolicLogprobsClient()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m client.chat_completion_raw(test_msgs, ll_base, logprobs=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mHyperbolicLogprobsClient.chat_completion_raw\u001b[39m\u001b[34m(self, messages, model, max_tokens, temperature, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m payload = {\n\u001b[32m     48\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m     49\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m     **kwargs,\n\u001b[32m     56\u001b[39m }\n\u001b[32m     59\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.post(\n\u001b[32m     60\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.base_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     61\u001b[39m     headers=headers,\n\u001b[32m     62\u001b[39m     json=payload,\n\u001b[32m     63\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/MATS_ICM/.venv/lib/python3.12/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    827\u001b[39m error_type = error_types.get(status_class, \u001b[33m\"\u001b[39m\u001b[33mInvalid status code\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '422 Unprocessable Entity' for url 'https://api.hyperbolic.xyz/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422"
     ]
    }
   ],
   "source": [
    "client = HyperbolicLogprobsClient()\n",
    "response = await client.chat_completion_raw(test_msgs, ll_base, logprobs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e3907d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal Hyperbolic API client - just get it working.\n",
    "\"\"\"\n",
    "import os\n",
    "import httpx\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "\n",
    "async def call_hyperbolic_raw(\n",
    "    messages: List[Dict[str, str]],\n",
    "    model: str,\n",
    "    api_key: str = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Minimal raw API call to Hyperbolic.\n",
    "    \n",
    "    Returns the full JSON response.\n",
    "    \"\"\"\n",
    "    api_key = api_key or os.getenv(\"HYPERBOLIC_API_KEY\")\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "    }\n",
    "    \n",
    "    # Minimal payload matching their example\n",
    "    payload = {\n",
    "        \"messages\": messages,\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 10,\n",
    "        \"temperature\": 0.0,\n",
    "        \"logprobs\": True,\n",
    "        \"echo\": True,\n",
    "        \"top_logprobs\": 2,\n",
    "    }\n",
    "    \n",
    "    async with httpx.AsyncClient(timeout=60.0) as client:\n",
    "        response = await client.post(\n",
    "            \"https://api.hyperbolic.xyz/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "\n",
    "# Test it\n",
    "async def test():\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Does 1+2=4? Reason about this then answer True or False.\"}\n",
    "    ]\n",
    "    \n",
    "    result = await call_hyperbolic_raw(\n",
    "        messages=messages,\n",
    "        # model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "        model=\"Qwen/Qwen3-Next-80B-A3B-Instruct\"# \"Qwen/Qwen2.5-Coder-32B-Instruct\",#\"openai/gpt-oss-20b\", #meta-llama/Meta-Llama-3.1-405B-Instruct\" \n",
    "        #\"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n",
    "    )\n",
    "    \n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "# Run: asyncio.run(test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a8b858ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'ddf4bee8930a486d9384801025fbf55a', 'object': 'chat.completion', 'created': 1762264452, 'model': 'Qwen/Qwen3-Next-80B-A3B-Instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Let's reason step by step:\\n\\nWe are asked\", 'reasoning_content': None, 'tool_calls': None}, 'logprobs': {'content': [{'token': 'Let', 'bytes': [76, 101, 116], 'logprob': -0.0005292683490552008, 'top_logprobs': [{'token': 'Let', 'bytes': [76, 101, 116], 'logprob': -0.0005292683490552008}, {'token': 'No', 'bytes': [78, 111], 'logprob': -7.629513740539551}]}, {'token': \"'s\", 'bytes': [39, 115], 'logprob': -0.007975872606039047, 'top_logprobs': [{'token': \"'s\", 'bytes': [39, 115], 'logprob': -0.007975872606039047}, {'token': '’s', 'bytes': [226, 128, 153, 115], 'logprob': -4.836585521697998}]}, {'token': ' reason', 'bytes': [32, 114, 101, 97, 115, 111, 110], 'logprob': -0.00014780859055463225, 'top_logprobs': [{'token': ' reason', 'bytes': [32, 114, 101, 97, 115, 111, 110], 'logprob': -0.00014780859055463225}, {'token': ' think', 'bytes': [32, 116, 104, 105, 110, 107], 'logprob': -9.77991771697998}]}, {'token': ' step', 'bytes': [32, 115, 116, 101, 112], 'logprob': -0.007671774365007877, 'top_logprobs': [{'token': ' step', 'bytes': [32, 115, 116, 101, 112], 'logprob': -0.007671774365007877}, {'token': ' about', 'bytes': [32, 97, 98, 111, 117, 116], 'logprob': -5.535147190093994}]}, {'token': ' by', 'bytes': [32, 98, 121], 'logprob': -6.198863957251888e-06, 'top_logprobs': [{'token': ' by', 'bytes': [32, 98, 121], 'logprob': -6.198863957251888e-06}, {'token': '-by', 'bytes': [45, 98, 121], 'logprob': -11.982194900512695}]}, {'token': ' step', 'bytes': [32, 115, 116, 101, 112], 'logprob': -5.960462772236497e-07, 'top_logprobs': [{'token': ' step', 'bytes': [32, 115, 116, 101, 112], 'logprob': -5.960462772236497e-07}, {'token': '-step', 'bytes': [45, 115, 116, 101, 112], 'logprob': -14.336939811706543}]}, {'token': ':\\n\\n', 'bytes': [58, 10, 10], 'logprob': -0.2600192129611969, 'top_logprobs': [{'token': ':\\n\\n', 'bytes': [58, 10, 10], 'logprob': -0.2600192129611969}, {'token': '.\\n\\n', 'bytes': [46, 10, 10], 'logprob': -1.4741953611373901}]}, {'token': 'We', 'bytes': [87, 101], 'logprob': -2.622600959512056e-06, 'top_logprobs': [{'token': 'We', 'bytes': [87, 101], 'logprob': -2.622600959512056e-06}, {'token': '-', 'bytes': [45], 'logprob': -13.68312931060791}]}, {'token': ' are', 'bytes': [32, 97, 114, 101], 'logprob': -0.002030455507338047, 'top_logprobs': [{'token': ' are', 'bytes': [32, 97, 114, 101], 'logprob': -0.002030455507338047}, {'token': ' know', 'bytes': [32, 107, 110, 111, 119], 'logprob': -6.353549003601074}]}, {'token': ' asked', 'bytes': [32, 97, 115, 107, 101, 100], 'logprob': -0.15813902020454407, 'top_logprobs': [{'token': ' asked', 'bytes': [32, 97, 115, 107, 101, 100], 'logprob': -0.15813902020454407}, {'token': ' given', 'bytes': [32, 103, 105, 118, 101, 110], 'logprob': -2.1031157970428467}]}]}, 'finish_reason': 'length', 'matched_stop': None}], 'usage': {'prompt_tokens': 25, 'total_tokens': 35, 'completion_tokens': 10, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'metadata': {'weight_version': 'default'}}\n",
      "{'content': [{'token': 'Let', 'bytes': [76, 101, 116], 'logprob': -0.0005292683490552008, 'top_logprobs': [{'token': 'Let', 'bytes': [76, 101, 116], 'logprob': -0.0005292683490552008}, {'token': 'No', 'bytes': [78, 111], 'logprob': -7.629513740539551}]}, {'token': \"'s\", 'bytes': [39, 115], 'logprob': -0.007975872606039047, 'top_logprobs': [{'token': \"'s\", 'bytes': [39, 115], 'logprob': -0.007975872606039047}, {'token': '’s', 'bytes': [226, 128, 153, 115], 'logprob': -4.836585521697998}]}, {'token': ' reason', 'bytes': [32, 114, 101, 97, 115, 111, 110], 'logprob': -0.00014780859055463225, 'top_logprobs': [{'token': ' reason', 'bytes': [32, 114, 101, 97, 115, 111, 110], 'logprob': -0.00014780859055463225}, {'token': ' think', 'bytes': [32, 116, 104, 105, 110, 107], 'logprob': -9.77991771697998}]}, {'token': ' step', 'bytes': [32, 115, 116, 101, 112], 'logprob': -0.007671774365007877, 'top_logprobs': [{'token': ' step', 'bytes': [32, 115, 116, 101, 112], 'logprob': -0.007671774365007877}, {'token': ' about', 'bytes': [32, 97, 98, 111, 117, 116], 'logprob': -5.535147190093994}]}, {'token': ' by', 'bytes': [32, 98, 121], 'logprob': -6.198863957251888e-06, 'top_logprobs': [{'token': ' by', 'bytes': [32, 98, 121], 'logprob': -6.198863957251888e-06}, {'token': '-by', 'bytes': [45, 98, 121], 'logprob': -11.982194900512695}]}, {'token': ' step', 'bytes': [32, 115, 116, 101, 112], 'logprob': -5.960462772236497e-07, 'top_logprobs': [{'token': ' step', 'bytes': [32, 115, 116, 101, 112], 'logprob': -5.960462772236497e-07}, {'token': '-step', 'bytes': [45, 115, 116, 101, 112], 'logprob': -14.336939811706543}]}, {'token': ':\\n\\n', 'bytes': [58, 10, 10], 'logprob': -0.2600192129611969, 'top_logprobs': [{'token': ':\\n\\n', 'bytes': [58, 10, 10], 'logprob': -0.2600192129611969}, {'token': '.\\n\\n', 'bytes': [46, 10, 10], 'logprob': -1.4741953611373901}]}, {'token': 'We', 'bytes': [87, 101], 'logprob': -2.622600959512056e-06, 'top_logprobs': [{'token': 'We', 'bytes': [87, 101], 'logprob': -2.622600959512056e-06}, {'token': '-', 'bytes': [45], 'logprob': -13.68312931060791}]}, {'token': ' are', 'bytes': [32, 97, 114, 101], 'logprob': -0.002030455507338047, 'top_logprobs': [{'token': ' are', 'bytes': [32, 97, 114, 101], 'logprob': -0.002030455507338047}, {'token': ' know', 'bytes': [32, 107, 110, 111, 119], 'logprob': -6.353549003601074}]}, {'token': ' asked', 'bytes': [32, 97, 115, 107, 101, 100], 'logprob': -0.15813902020454407, 'top_logprobs': [{'token': ' asked', 'bytes': [32, 97, 115, 107, 101, 100], 'logprob': -0.15813902020454407}, {'token': ' given', 'bytes': [32, 103, 105, 118, 101, 110], 'logprob': -2.1031157970428467}]}]}\n"
     ]
    }
   ],
   "source": [
    "r = await test()\n",
    "print(r['choices'][0]['logprobs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8110c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7681780e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"id\": \"oHvBvpn-4YNCb4-9994822a3c53989b\",\n",
      " \"object\": \"chat.completion\",\n",
      " \"created\": 1762263471,\n",
      " \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
      " \"choices\": [\n",
      "  {\n",
      "   \"index\": 0,\n",
      "   \"logprobs\": {\n",
      "    \"tokens\": [\n",
      "     \"The\",\n",
      "     \" city\",\n",
      "     \" that\"\n",
      "    ],\n",
      "    \"token_logprobs\": [\n",
      "     -0.640625,\n",
      "     -0.044921875,\n",
      "     -0.00063323975\n",
      "    ],\n",
      "    \"token_ids\": [\n",
      "     791,\n",
      "     3363,\n",
      "     430\n",
      "    ],\n",
      "    \"top_logprobs\": [\n",
      "     {\n",
      "      \"The\": -0.640625,\n",
      "      \"New\": -0.765625\n",
      "     },\n",
      "     {\n",
      "      \" city\": -0.044921875,\n",
      "      \" top\": -3.796875\n",
      "     },\n",
      "     {\n",
      "      \" that\": -0.00063323975,\n",
      "      \" of\": -7.375\n",
      "     }\n",
      "    ]\n",
      "   },\n",
      "   \"seed\": 1547669890996867600,\n",
      "   \"finish_reason\": \"length\",\n",
      "   \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The city that\",\n",
      "    \"tool_calls\": []\n",
      "   }\n",
      "  }\n",
      " ],\n",
      " \"prompt\": [],\n",
      " \"usage\": {\n",
      "  \"prompt_tokens\": 43,\n",
      "  \"completion_tokens\": 3,\n",
      "  \"total_tokens\": 46,\n",
      "  \"cached_tokens\": 0\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from together import Together\n",
    "import json\n",
    "\n",
    "client = Together()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"What are the top 3 things to do in New York?\",\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=3,\n",
    "    logprobs=2,\n",
    ")\n",
    "\n",
    "print(json.dumps(completion.model_dump(), indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f3714136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionChoicesData(index=0, logprobs=LogprobsPart(tokens=['The', ' city', ' that', ' never', ' sleeps', '!', ' New', ' York', ' is', ' a'], token_logprobs=[-0.640625, -0.044921875, -0.00063323975, -2.0742416e-05, -0.00012874603, -0.58203125, -0.0026397705, -5.9604645e-07, -0.69921875, -0.013061523], token_ids=[791, 3363, 430, 2646, 72490, 0, 1561, 4356, 374, 264], top_logprobs=[{'The': -0.640625}, {' city': -0.044921875}, {' that': -0.00063323975}, {' never': -2.0742416e-05}, {' sleeps': -0.00012874603}, {'!': -0.58203125}, {' New': -0.0026397705}, {' York': -5.9604645e-07}, {' is': -0.69921875}, {' a': -0.013061523}]), seed=1335840185419321600, finish_reason=<FinishReason.Length: 'length'>, message=ChatCompletionMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='The city that never sleeps! New York is a', tool_calls=[]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b796a37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
